{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f62dc73-df18-4548-bc0d-aea2a125fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.stats.multitest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# disable warnings, use w caution\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# project specific libs\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b42930-bcde-48f3-9753-8779e1aa11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project specific path\n",
    "path = '/Users/KevinBu/Desktop/clemente_lab/Projects/oa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3013747-2e0e-412a-b719-b2a6a28afbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BarcodeSequence</th>\n",
       "      <th>LinkerPrimerSequence</th>\n",
       "      <th>Separate</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Together</th>\n",
       "      <th>ContactEmail</th>\n",
       "      <th>ContactName</th>\n",
       "      <th>PrimaryInvestigator</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>RawDataNotes</th>\n",
       "      <th>...</th>\n",
       "      <th>broccoli</th>\n",
       "      <th>Garbanzo_beans</th>\n",
       "      <th>pork</th>\n",
       "      <th>beef</th>\n",
       "      <th>burger</th>\n",
       "      <th>Total_omega3</th>\n",
       "      <th>Adherence_omega3</th>\n",
       "      <th>Total_omega6</th>\n",
       "      <th>Adherence_omega6</th>\n",
       "      <th>Total_o3_o6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#SampleID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OAD-001.pre.stool</th>\n",
       "      <td>TTCAGTTCGTTA</td>\n",
       "      <td>CCGGACTACHVGGGTWTCTAAT</td>\n",
       "      <td>All</td>\n",
       "      <td>pre</td>\n",
       "      <td>OAD-001.pre.stool.guma.plate313</td>\n",
       "      <td>rebecca.blank@nyulangone.org</td>\n",
       "      <td>Rebecca Blank</td>\n",
       "      <td>Jose Scher</td>\n",
       "      <td>NonVA</td>\n",
       "      <td>OAD-001.pre.stool.guma.plate313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAD-001.post.stool</th>\n",
       "      <td>CGGCCAGAAGCA</td>\n",
       "      <td>CCGGACTACHVGGGTWTCTAAT</td>\n",
       "      <td>All</td>\n",
       "      <td>post</td>\n",
       "      <td>OAD-001.post.stool.guma.plate313</td>\n",
       "      <td>rebecca.blank@nyulangone.org</td>\n",
       "      <td>Rebecca Blank</td>\n",
       "      <td>Jose Scher</td>\n",
       "      <td>NonVA</td>\n",
       "      <td>OAD-001.post.stool.guma.plate313</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>72.0</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAD-003.pre.stool</th>\n",
       "      <td>GACGTTAAGAAT</td>\n",
       "      <td>CCGGACTACHVGGGTWTCTAAT</td>\n",
       "      <td>All</td>\n",
       "      <td>pre</td>\n",
       "      <td>OAD-003.pre.stool.guma.plate313</td>\n",
       "      <td>rebecca.blank@nyulangone.org</td>\n",
       "      <td>Rebecca Blank</td>\n",
       "      <td>Jose Scher</td>\n",
       "      <td>NonVA</td>\n",
       "      <td>OAD-003.pre.stool.guma.plate313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>53.2</td>\n",
       "      <td>High adherence</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAD-003.post.stool</th>\n",
       "      <td>TCGCTACAGATG</td>\n",
       "      <td>CCGGACTACHVGGGTWTCTAAT</td>\n",
       "      <td>All</td>\n",
       "      <td>post</td>\n",
       "      <td>OAD-003.post.stool.guma.plate313</td>\n",
       "      <td>rebecca.blank@nyulangone.org</td>\n",
       "      <td>Rebecca Blank</td>\n",
       "      <td>Jose Scher</td>\n",
       "      <td>NonVA</td>\n",
       "      <td>OAD-003.post.stool.guma.plate313</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>108.0</td>\n",
       "      <td>High adherence</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAD-004.pre.stool</th>\n",
       "      <td>ATGGGACCTTCA</td>\n",
       "      <td>CCGGACTACHVGGGTWTCTAAT</td>\n",
       "      <td>All</td>\n",
       "      <td>pre</td>\n",
       "      <td>OAD-004.pre.stool.guma.plate313</td>\n",
       "      <td>rebecca.blank@nyulangone.org</td>\n",
       "      <td>Rebecca Blank</td>\n",
       "      <td>Jose Scher</td>\n",
       "      <td>NonVA</td>\n",
       "      <td>OAD-004.pre.stool.guma.plate313</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Low adherence</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BarcodeSequence    LinkerPrimerSequence Separate Timepoint  \\\n",
       "#SampleID                                                                       \n",
       "OAD-001.pre.stool     TTCAGTTCGTTA  CCGGACTACHVGGGTWTCTAAT      All       pre   \n",
       "OAD-001.post.stool    CGGCCAGAAGCA  CCGGACTACHVGGGTWTCTAAT      All      post   \n",
       "OAD-003.pre.stool     GACGTTAAGAAT  CCGGACTACHVGGGTWTCTAAT      All       pre   \n",
       "OAD-003.post.stool    TCGCTACAGATG  CCGGACTACHVGGGTWTCTAAT      All      post   \n",
       "OAD-004.pre.stool     ATGGGACCTTCA  CCGGACTACHVGGGTWTCTAAT      All       pre   \n",
       "\n",
       "                                            Together  \\\n",
       "#SampleID                                              \n",
       "OAD-001.pre.stool    OAD-001.pre.stool.guma.plate313   \n",
       "OAD-001.post.stool  OAD-001.post.stool.guma.plate313   \n",
       "OAD-003.pre.stool    OAD-003.pre.stool.guma.plate313   \n",
       "OAD-003.post.stool  OAD-003.post.stool.guma.plate313   \n",
       "OAD-004.pre.stool    OAD-004.pre.stool.guma.plate313   \n",
       "\n",
       "                                    ContactEmail    ContactName  \\\n",
       "#SampleID                                                         \n",
       "OAD-001.pre.stool   rebecca.blank@nyulangone.org  Rebecca Blank   \n",
       "OAD-001.post.stool  rebecca.blank@nyulangone.org  Rebecca Blank   \n",
       "OAD-003.pre.stool   rebecca.blank@nyulangone.org  Rebecca Blank   \n",
       "OAD-003.post.stool  rebecca.blank@nyulangone.org  Rebecca Blank   \n",
       "OAD-004.pre.stool   rebecca.blank@nyulangone.org  Rebecca Blank   \n",
       "\n",
       "                   PrimaryInvestigator Cohort  \\\n",
       "#SampleID                                       \n",
       "OAD-001.pre.stool           Jose Scher  NonVA   \n",
       "OAD-001.post.stool          Jose Scher  NonVA   \n",
       "OAD-003.pre.stool           Jose Scher  NonVA   \n",
       "OAD-003.post.stool          Jose Scher  NonVA   \n",
       "OAD-004.pre.stool           Jose Scher  NonVA   \n",
       "\n",
       "                                        RawDataNotes  ... broccoli  \\\n",
       "#SampleID                                             ...            \n",
       "OAD-001.pre.stool    OAD-001.pre.stool.guma.plate313  ...      0.0   \n",
       "OAD-001.post.stool  OAD-001.post.stool.guma.plate313  ...      4.0   \n",
       "OAD-003.pre.stool    OAD-003.pre.stool.guma.plate313  ...      0.0   \n",
       "OAD-003.post.stool  OAD-003.post.stool.guma.plate313  ...      4.0   \n",
       "OAD-004.pre.stool    OAD-004.pre.stool.guma.plate313  ...      2.8   \n",
       "\n",
       "                   Garbanzo_beans pork  beef burger Total_omega3  \\\n",
       "#SampleID                                                          \n",
       "OAD-001.pre.stool             0.0  0.0   0.0    0.0          NaN   \n",
       "OAD-001.post.stool            0.0  0.0   0.0    0.0         48.0   \n",
       "OAD-003.pre.stool             0.0  0.0  14.0    0.0         16.8   \n",
       "OAD-003.post.stool            0.0  0.0   0.0    0.0         48.0   \n",
       "OAD-004.pre.stool             0.0  0.0   0.0    0.0          2.8   \n",
       "\n",
       "                   Adherence_omega3 Total_omega6 Adherence_omega6 Total_o3_o6  \n",
       "#SampleID                                                                      \n",
       "OAD-001.pre.stool     Low adherence          NaN    Low adherence         0.0  \n",
       "OAD-001.post.stool    Low adherence         72.0    Low adherence       131.0  \n",
       "OAD-003.pre.stool     Low adherence         53.2   High adherence        75.0  \n",
       "OAD-003.post.stool    Low adherence        108.0   High adherence       171.0  \n",
       "OAD-004.pre.stool     Low adherence         28.0    Low adherence        35.8  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from AC Q2 run of merged saliva stool\n",
    "df_map = pd.read_csv(path + 'inputs/Qiime2_0/qiime_mapping_file.tsv', sep='\\t', index_col=0)\n",
    "q2_row = df_map.loc['#q2:types',:]\n",
    "df_map = df_map.drop('#q2:types')\n",
    "\n",
    "# change index so it matches metadata file\n",
    "df_map.index = df_map.index.map(lambda x: x.split('.guma')[0])\n",
    "\n",
    "# drop MOC and elution buffer\n",
    "df_map = df_map.drop(['MOC.320','elutionbuffer.plate313'])\n",
    "\n",
    "# grab metadata\n",
    "df_meta = pd.read_csv(path + 'inputs/Metadata_OA.csv')\n",
    "\n",
    "# rename 'Run_ID_Saliva' to be correct\n",
    "df_meta['Timepoints'] = df_meta['Timepoints'].apply(lambda x: 'pre' if x == '0' else 'post')\n",
    "df_meta['Patient_ID'] = df_meta['Patient_ID'].apply(lambda x: x[:-3])  \n",
    "df_meta['Study_ID'] = df_meta['Study_ID'].apply(lambda x: x.split('_')[0][-3:]) \n",
    "\n",
    "# create per sample type mapping files\n",
    "type_to_ST = {'saliva':'Saliva','stool':'fecal'}\n",
    "type_to_df_map = {}\n",
    "\n",
    "# split into specimen type\n",
    "for t in type_to_ST:\n",
    "    # subset on specimen type\n",
    "    df_map_type = df_map[df_map['SpecimenType'] == type_to_ST[t]]\n",
    "\n",
    "    # as to not overwrite df meta\n",
    "    df_meta_type = df_meta.copy()\n",
    "\n",
    "    # create new sample ID for specimen type and set as index\n",
    "    df_meta_type['#SampleID'] = df_meta['Patient_ID'] + '-' + df_meta['Study_ID'] + '.' + df_meta['Timepoints'] + '.' + t\n",
    "    df_meta_type = df_meta_type.set_index('#SampleID')\n",
    "\n",
    "    # create full mapping file\n",
    "    df_map_type = pd.concat([df_map_type, df_meta_type],axis=1)\n",
    "\n",
    "    # use only sequenced samples\n",
    "    df_map_type = df_map_type.dropna(how='any',subset='BarcodeSequence')\n",
    "\n",
    "    # drop all na columns\n",
    "    df_map_type = df_map_type.dropna(how='all',axis=1)\n",
    "\n",
    "    # drop VAD OA 015 because misdx with PsA not OA\n",
    "    if t == 'saliva':\n",
    "        df_map_type = df_map_type.drop(['VAOAD-015.pre.saliva','VAOAD-015.post.saliva'])\n",
    "    if t == 'stool':\n",
    "        df_map_type = df_map_type.drop(['VAOAD-015.pre.stool','VAOAD-015.post.stool'])\n",
    "\n",
    "    # populate dict of mapping files\n",
    "    type_to_df_map[t] = df_map_type\n",
    "\n",
    "    # export for q2\n",
    "    df_q2_type = pd.concat([q2_row.to_frame().T, df_map_type])\n",
    "    df_q2_type.index.name = '#SampleID'\n",
    "    df_q2_type.iloc[0,:] = 'categorical'\n",
    "    df_q2_type.to_csv(path + 'inputs/qiime_mapping_file_' + t + '.tsv', sep='\\t')\n",
    "    df_q2_type = df_q2_type[df_q2_type['Adherece_antiinflam'].isin(['Moderate adherence', 'High adherence','categorical'])]\n",
    "    df_q2_type.to_csv(path + 'inputs/qiime_mapping_file_' + t + '_adh.tsv', sep='\\t')\n",
    "    df_q2_type = df_q2_type[df_q2_type['Adherece_antiinflam'].isin(['Moderate adherence', 'High adherence','categorical'])]\n",
    "    df_q2_resp = df_q2_type[df_q2_type['WOMAC_P_Response'].isin(['categorical', 'Response'])]\n",
    "    df_q2_resp.to_csv(path + 'inputs/qiime_mapping_file_' + t + '_adh_response.tsv', sep='\\t')\n",
    "    df_q2_nonresp = df_q2_type[df_q2_type['WOMAC_P_Response'].isin(['categorical', 'No Response'])]\n",
    "    df_q2_nonresp.to_csv(path + 'inputs/qiime_mapping_file_' + t + '_adh_noresponse.tsv', sep='\\t')\n",
    "\n",
    "type_to_df_map['stool'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4ef90-0416-4803-a5b2-d730031630c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Hypothesis 1: There will be a measurable difference in WOMAC pain response scores and \n",
    "# other outcomes from baseline to after the dietary intervention.\n",
    "###\n",
    "\n",
    "# outcome variables\n",
    "outcomes = ['VAS_Pt', 'VAS_overall', 'WOMAC_pain', 'WOMAC_stiffness', 'WOMAC_activity', 'WOMAC_total', 'Pain_DETECT', \n",
    "            'CES_D', 'Helplesness', 'Magnification', 'Rumination', 'PCS_EN', 'Sleep_distrubance', 'PASE_walk', 'PASE_light', \n",
    "            #'PASE_gardening', # Where did this go? gardening_improve is binary\n",
    "            'BMI']\n",
    "\n",
    "\n",
    "# hypothesis 1\n",
    "# create a new df_meta\n",
    "df_meta = pd.read_csv(path + 'inputs/Metadata_OA.csv')\n",
    "\n",
    "# rename 'Run_ID_Saliva' to be correct\n",
    "df_meta['Timepoints'] = df_meta['Timepoints'].apply(lambda x: 'pre' if x == '0' else 'post')\n",
    "df_meta['Patient_ID'] = df_meta['Patient_ID'].apply(lambda x: x[:-3])  \n",
    "df_meta['Study_ID'] = df_meta['Study_ID'].apply(lambda x: x.split('_')[0][-3:]) \n",
    "df_meta['#SampleID'] = df_meta['Patient_ID'] + '-' + df_meta['Study_ID'] + '.' + df_meta['Timepoints'] + '.stool'\n",
    "df_meta = df_meta.set_index('#SampleID')\n",
    "\n",
    "# convert % to floats for calculations down the road\n",
    "bin = []\n",
    "cont = []\n",
    "df_paired_os = []\n",
    "for w in outcomes:\n",
    "    df_w = df_meta[w]\n",
    "    if df_w.nunique() > 2: # do spearman\n",
    "        df_meta[w] = df_meta[w].astype(str).str.replace('%','').astype(float).values\n",
    "        cont.append(w)\n",
    "    else:\n",
    "        bin.append(w)\n",
    "\n",
    "    # compute difference and store it\n",
    "    df_md = df_meta.copy()\n",
    "    df_md['SubjectID'] = df_md['Patient_ID'] + df_md['Study_ID']\n",
    "    \n",
    "    # first drop unpaired samples\n",
    "    s_remove = []\n",
    "    for s in list(df_md['SubjectID'].values):\n",
    "        if len(df_md[df_md['SubjectID'] == s]) != 2:\n",
    "            s_remove.append(s)\n",
    "    df_md = df_md.loc[~df_md['SubjectID'].isin(s_remove),:] # careful not to use ([s_remove])\n",
    "    \n",
    "    # set vars\n",
    "    group_var = 'Timepoints'\n",
    "    pair_var = 'SubjectID'\n",
    "    groups = ['pre','post']\n",
    "    \n",
    "    # get paired per indiv pair\n",
    "    pair_to_diff = {}\n",
    "    for p in list(df_md[pair_var].values):\n",
    "        df = df_md[df_md[pair_var] == p]\n",
    "        t0 = float(df[df[group_var] == groups[0]][w].values)\n",
    "        tf = float(df[df[group_var] == groups[1]][w].values)\n",
    "        pair_to_diff[p] = tf - t0\n",
    "    \n",
    "    df_paired_o = pd.DataFrame.from_dict(pair_to_diff, orient='index', columns=[w + '_diff'])\n",
    "    df_paired_os.append(df_paired_o)\n",
    "\n",
    "df_meta_paired = pd.concat([*df_paired_os], axis=1)    \n",
    "\n",
    "print(bin)\n",
    "print(cont)\n",
    "\n",
    "# split into all and mod high only\n",
    "for a in ['all','modhigh','low']:\n",
    "    print(a)\n",
    "    if a == 'all':\n",
    "        job = 'jobs03'\n",
    "    if a == 'modhigh':\n",
    "        job = 'jobs02'\n",
    "        df_meta = df_meta[df_meta['Adherece_antiinflam'].isin(['Moderate adherence', 'High adherence'])]\n",
    "    if a == 'low':\n",
    "        job = 'jobs02a'\n",
    "        df_meta = df_meta[df_meta['Adherece_antiinflam'] == 'Low adherence']\n",
    "        \n",
    "    print(len(df_meta))\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['var','effect','pval','stat'])\n",
    "    # do post treatment vals of binary vars differ from pre treatment 'unpaired'\n",
    "    for b in bin:\n",
    "        ct_table_ind=pd.crosstab(df_meta[\"Timepoints\"],df_meta[b])\n",
    "        chi2_stat, p, dof, expected = scipy.stats.chi2_contingency(ct_table_ind)\n",
    "        row=pd.DataFrame.from_dict({'var': [b],'effect':[chi2_stat],'pval':[p],'stat':['chi2']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "    \n",
    "    # fishers exact\n",
    "    for b in bin:\n",
    "        ct_table_ind=pd.crosstab(df_meta[\"Timepoints\"],df_meta[b])\n",
    "        fisher, p = scipy.stats.fisher_exact(ct_table_ind)\n",
    "        row=pd.DataFrame.from_dict({'var': [b],'effect':[t],'pval':[p],'stat':['fisher']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "        \n",
    "    # do post treatment vals of continuous vars differ from pre treatment unpaired\n",
    "    df_pre = df_meta[df_meta['Timepoints'] == 'pre']\n",
    "    df_post = df_meta[df_meta['Timepoints'] == 'post']\n",
    "    for c in cont:\n",
    "        try:\n",
    "            W,p = scipy.stats.mannwhitneyu(x=df_pre[c].values,y=df_post[c].values, nan_policy='omit')\n",
    "        except:\n",
    "            W,p = 0, 1\n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[W],'pval':[p],'stat':['mwu']})\n",
    "        if p < 0.05:\n",
    "            ax = sns.boxplot(data=df_meta, x='Timepoints', y=c, orient='v')\n",
    "            sns.swarmplot(data=df_meta, x='Timepoints', y=c, palette='dark:grey', hue=None, orient='v')\n",
    "        \n",
    "            # ax.axes.set_title(\"Title\",fontsize=48)\n",
    "            ax.set_ylabel(c,fontsize=16)\n",
    "            ax.set_xlabel('Timepoints',fontsize=16)                \n",
    "            ax.tick_params(labelsize=16)\n",
    "            sns.despine()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/' + job + '/mwu_' + c  + '.pdf')\n",
    "            plt.close()            \n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[W],'pval':[p],'stat':['MWU']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "        \n",
    "    df_pre = df_meta[df_meta['Timepoints'] == 'pre']\n",
    "    df_post = df_meta[df_meta['Timepoints'] == 'post']\n",
    "    for c in cont:\n",
    "        t,p = scipy.stats.ttest_ind(a=df_pre[c].values,b=df_post[c].values, nan_policy='omit')\n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[t],'pval':[p],'stat':['ttest']})\n",
    "        if p < 0.05:\n",
    "            ax = sns.boxplot(data=df_meta, x='Timepoints', y=c, orient='v')\n",
    "            sns.swarmplot(data=df_meta, x='Timepoints', y=c, palette='dark:grey', hue=None, orient='v')\n",
    "        \n",
    "            # ax.axes.set_title(\"Title\",fontsize=48)\n",
    "            ax.set_ylabel(c,fontsize=16)\n",
    "            ax.set_xlabel('Timepoints',fontsize=16)                \n",
    "            ax.tick_params(labelsize=16)\n",
    "            sns.despine()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/' + job + '/tt_' + c  + '.pdf')\n",
    "            plt.close()            \n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[t],'pval':[p],'stat':['ttest']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "\n",
    "    # unpaired and then paired\n",
    "    df_pre = df_meta[df_meta['Timepoints'] == 'pre']\n",
    "    df_post = df_meta[df_meta['Timepoints'] == 'post']\n",
    "    for c in cont:\n",
    "        W,p = scipy.stats.wilcoxon(x=df_pre[c].values,y=df_post[c].values, nan_policy='omit')\n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[W],'pval':[p],'stat':['WSR']})\n",
    "        if p < 0.05:\n",
    "            ax = sns.boxplot(data=df_meta, x='Timepoints', y=c, orient='v')\n",
    "            sns.swarmplot(data=df_meta, x='Timepoints', y=c, palette='dark:grey', hue=None, orient='v')\n",
    "        \n",
    "            # ax.axes.set_title(\"Title\",fontsize=48)\n",
    "            ax.set_ylabel(c,fontsize=16)\n",
    "            ax.set_xlabel('Timepoints',fontsize=16)                \n",
    "            ax.tick_params(labelsize=16)\n",
    "            sns.despine()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/' + job + '/wsr_' + c  + '.pdf')\n",
    "            plt.close()            \n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[W],'pval':[p],'stat':['WSR']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "\n",
    "    df_pre = df_meta[df_meta['Timepoints'] == 'pre']\n",
    "    df_post = df_meta[df_meta['Timepoints'] == 'post']\n",
    "    for c in cont:\n",
    "        t,p = scipy.stats.ttest_rel(a=df_pre[c].values,b=df_post[c].values, nan_policy='omit')\n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[W],'pval':[p],'stat':['pairedt']})\n",
    "        if p < 0.05:\n",
    "            ax = sns.boxplot(data=df_meta, x='Timepoints', y=c, orient='v')\n",
    "            sns.swarmplot(data=df_meta, x='Timepoints', y=c, palette='dark:grey', hue=None, orient='v')\n",
    "        \n",
    "            # ax.axes.set_title(\"Title\",fontsize=48)\n",
    "            ax.set_ylabel(c,fontsize=16)\n",
    "            ax.set_xlabel('Timepoints',fontsize=16)                \n",
    "            ax.tick_params(labelsize=16)\n",
    "            sns.despine()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/' + job + '/pairedt_' + c  + '.pdf')\n",
    "            plt.close()          \n",
    "        row=pd.DataFrame.from_dict({'var': [c],'effect':[t],'pval':[p],'stat':['pairedt']})\n",
    "        df_results = pd.concat([df_results, row])\n",
    "\n",
    "    df_results.to_csv(path + 'outputs/' + job + '/outcome_testing.tsv', sep='\\t')\n",
    "df_results.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1b172-db26-452f-b13e-a671b23d4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are only 4 samples w low adh so probably just drop from all analysis, too small to use as a control group\n",
    "df_meta = pd.read_csv(path + 'inputs/Metadata_OA.csv')\n",
    "# df = df_meta[df_meta['Adherece_antiinflam'].isnull()]\n",
    "# df = df_meta[df_meta['Adherece_antiinflam'] != 'Low adherence']\n",
    "df = df_meta[df_meta['Adherece_antiinflam'] == 'Low adherence']\n",
    "# df['Patient_ID'].unique()\n",
    "# df_meta\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b4824-3c24-485e-8705-5daf4a85d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Hypothesis 2: There will be an association between oral and gut microbiome and pain outcomes\n",
    "###\n",
    "# construct alpha, beta and paired alpha dataframes\n",
    "g_to_dfd = {}\n",
    "g_test = ['stool','saliva_adh', 'stool_adh', 'saliva']\n",
    "\n",
    "for g in g_test:\n",
    "    # maps diversity type to dataframe\n",
    "    g_to_dfd[g] = {}\n",
    "\n",
    "    # get alpha diversities\n",
    "    df_alpha = pd.read_csv(path + 'outputs/Qiime2_' + g + '/metadata.tsv', sep='\\t', index_col=0)\n",
    "    df_alpha = df_alpha.drop('#q2:types')\n",
    "    df_alpha['SubjectID'] = df_alpha['Patient_ID'] + df_alpha['Study_ID']\n",
    "    df_alpha = df_alpha[['SubjectID', 'Timepoints', 'shannon_entropy']]\n",
    "    g_to_dfd[g]['alpha'] = df_alpha\n",
    "\n",
    "    # get paired alpha div, first drop unpaired samples\n",
    "    s_remove = []\n",
    "    for s in list(df_alpha['SubjectID'].values):\n",
    "        if len(df_alpha[df_alpha['SubjectID'] == s]) != 2:\n",
    "            s_remove.append(s)\n",
    "    df_alpha = df_alpha.loc[~df_alpha['SubjectID'].isin(s_remove),:] # careful not to use ([s_remove])\n",
    "    \n",
    "    # set vars\n",
    "    alpha_metric = 'shannon_entropy'\n",
    "    group_var = 'Timepoints'\n",
    "    pair_var = 'SubjectID'\n",
    "    groups = ['pre','post']\n",
    "    \n",
    "    # get paired per indiv pair\n",
    "    pair_to_diff = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        df = df_alpha[df_alpha[pair_var] == p]\n",
    "        alpha_0 = float(df[df[group_var] == groups[0]][alpha_metric].values)\n",
    "        alpha_1 = float(df[df[group_var] == groups[1]][alpha_metric].values)\n",
    "        pair_to_diff[p] = alpha_1 - alpha_0\n",
    "    \n",
    "    df_paired_alpha = pd.DataFrame.from_dict(pair_to_diff, orient='index', columns=[alpha_metric + '_diff'])\n",
    "    g_to_dfd[g]['paired_alpha'] = df_paired_alpha\n",
    "\n",
    "    # get beta div\n",
    "    df_beta = pd.read_csv(path + 'outputs/Qiime2_' + g + '/core_metrics_results/distance-matrix.tsv',\n",
    "                              sep='\\t', index_col=0)\n",
    "        \n",
    "    # grab twin to pair dict\n",
    "    pair_to_ids = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        df = df_alpha[df_alpha[pair_var] == p]\n",
    "        id_0 = str(df[df[group_var] == groups[0]].index.values[0])\n",
    "        id_1 = str(df[df[group_var] == groups[1]].index.values[0])\n",
    "        pair_to_ids[p] = (id_0, id_1)\n",
    "    \n",
    "    # get distances for each twin pair per beta div matrix    \n",
    "    pair_to_dist = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        id_0, id_1 = pair_to_ids[p]\n",
    "        pair_to_dist[p] = df_beta.loc[id_0, id_1]\n",
    "    \n",
    "    df_paired_beta = pd.DataFrame.from_dict(pair_to_dist, orient='index', columns=['Unweighted_Unifrac'])\n",
    "    g_to_dfd[g]['paired_beta'] = df_paired_beta\n",
    "\n",
    "\n",
    "# compute paired differences in pain\n",
    "        # compute the paired differences\n",
    "\n",
    "\n",
    "g_to_dfd['stool']['paired_beta'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f38ff-41d6-4f31-94ab-69879de56109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for each sample type for (1) all and (2) high adh only\n",
    "# (A) Chisq of quartiles with adherence \n",
    "# (B) MWU/TT unpaired of outcomes against distance\n",
    "\n",
    "def chisq_of_df_cols(df, c1, c2):\n",
    "    groupsizes = df.groupby([c1, c2]).size()\n",
    "    ctsum = groupsizes.unstack(c1)\n",
    "    # fillna(0) is necessary to remove any NAs which will cause exceptions\n",
    "    return(scipy.stats.chi2_contingency(ctsum.fillna(0)))\n",
    "\n",
    "d_to_metric = {\n",
    "    'alpha': 'shannon_entropy',\n",
    "}\n",
    "group_var = 'Adherece_antiinflam'\n",
    "\n",
    "dfd_to_merge = {}\n",
    "g_test = ['stool','saliva_adh', 'stool_adh', 'saliva']\n",
    "\n",
    "for g in g_test:\n",
    "    dfd_to_merge[g] = {}\n",
    "\n",
    "gs = []\n",
    "ds = []\n",
    "os = []\n",
    "stats = []\n",
    "ts = []\n",
    "ps = []\n",
    "\n",
    "arr = [gs,ds,os,stats,ts,ps]\n",
    "\n",
    "def append_results(arr, val):\n",
    "    for a,v in zip(arr,val):\n",
    "        a.append(v)\n",
    "    return arr\n",
    "\n",
    "# for each sample type, grab relevant mapping file\n",
    "# g_test = saliva, saliva_adh, stool, etc.\n",
    "for g in g_test:\n",
    "    for d in d_to_metric:\n",
    "        # grab relevant diversity df\n",
    "        df_div = g_to_dfd[g][d]\n",
    "\n",
    "        if d == 'alpha':\n",
    "            # df_div = df_div[df_div['Timepoints'] == 'pre']\n",
    "            # df_div = df_div.set_index('SubjectID').drop('Timepoints',axis=1)\n",
    "            # df_div = df_div.set_index('SubjectID').drop('Timepoints',axis=1)\n",
    "            df_div['shannon_entropy'] = df_div['shannon_entropy'].astype(float)\n",
    "\n",
    "        # merge with df of metadata var        \n",
    "        # grab relevant sample IDs\n",
    "        # g = saliva_adh\n",
    "        idx = 'Run_ID_' + g.split('_')[0].capitalize()\n",
    "        df_meta_sub = df_meta.dropna(subset=idx)\n",
    "        df_meta_sub = df_meta_sub.set_index(idx)\n",
    "        df_merge = pd.concat([df_meta_sub,df_div],axis=1)\n",
    "\n",
    "        # subset on adh only\n",
    "        if g.split('_')[-1] == 'adh':\n",
    "            df_merge = df_merge[df_merge[group_var].isin(['Moderate adherence', 'High adherence'])]\n",
    "\n",
    "        # test association of div with outcomes\n",
    "        for o in outcomes:            \n",
    "            if o in 'PASE_light' or o in 'PASE_walk' or o in 'Magnification': # only 2\n",
    "                df_merge[o + '_quartiles'] = np.where(df_merge[o]==0, 'bottom', 'top')\n",
    "            #elif o in 'Magnification': # only 3 \n",
    "            #    df_merge[o + '_quartiles'] = pd.qcut(df_merge[o].values, 3, labels = ['top','mid','bottom'])#, duplicates='drop')  \n",
    "            else:\n",
    "                df_merge[o + '_quartiles'] = pd.qcut(df_merge[o].values, 4, labels = ['top','midtop','midbot','bottom'])#, duplicates='drop')  \n",
    "                \n",
    "            # test association of adherence with pain outcomes\n",
    "            div_metric = d_to_metric[d]\n",
    "            x,p,dof,ef = chisq_of_df_cols(df_merge, group_var, o + '_quartiles')\n",
    "            arr = append_results(arr, ['metadata',group_var,o,'chisq',x,p])\n",
    "            \n",
    "            ax = sns.boxplot(data=df_merge, x=o + '_quartiles', y=div_metric)\n",
    "            sns.swarmplot(data=df_merge, x=o + '_quartiles', y=div_metric, palette='dark:grey')\n",
    "            sns.despine()\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/Qiime2_' + g + '/quartiles_nondiff_' + o + '_' + d + '.pdf')\n",
    "            plt.close()          \n",
    "        \n",
    "            u, p = scipy.stats.mannwhitneyu(df_merge[df_merge[o + '_quartiles'] == 'top'][div_metric].values, \n",
    "                                            df_merge[df_merge[o + '_quartiles'] == 'bottom'][div_metric].values, \n",
    "                                            nan_policy='omit')\n",
    "\n",
    "            arr = append_results(arr, [g,d,o,'mwu',t,p])\n",
    "\n",
    "            t, p = scipy.stats.ttest_ind(df_merge[df_merge[o + '_quartiles'] == 'top'][div_metric].values, \n",
    "                                            df_merge[df_merge[o + '_quartiles'] == 'bottom'][div_metric].values, \n",
    "                                            nan_policy='omit')\n",
    "\n",
    "            arr = append_results(arr, [g,d,o,'tt',t,p])\n",
    "\n",
    "        # save results\n",
    "        dfd_to_merge[g][d] = df_merge\n",
    "\n",
    "        # export to Q2\n",
    "        # df_q2_type = df_merge.set_index(['Together'])\n",
    "        df_q2_type = df_merge.copy()\n",
    "        q2_row = pd.Series(data=['categorical' for i in range(len(df_merge.columns))], \n",
    "                           index=list(df_merge.columns.values), dtype=str, name='#q2:types')\n",
    "        df_q2_type = pd.concat([q2_row.to_frame().T, df_q2_type])\n",
    "        df_q2_type.index.name = '#SampleID'\n",
    "        df_q2_type.index = df_q2_type.index.map(lambda x: x.split('.guma')[0])\n",
    "        df_q2_type.to_csv(path + 'inputs/qiime_mapping_file_' + d + '_' + g + 'aggregate_outcomes.tsv', sep='\\t')\n",
    "\n",
    "df_results = pd.DataFrame.from_dict({\n",
    "    'group': gs,\n",
    "    'div': ds,\n",
    "    'outcome': os,\n",
    "    'statistic': stats,\n",
    "    'test_stat': ts,\n",
    "    'pval': ps\n",
    "})\n",
    "df_results.to_csv(path + 'outputs/df_results_aggregate.tsv', sep='\\t')\n",
    "\n",
    "# df_results.head()\n",
    "df_results[df_results['pval'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f26972-623f-459c-93fd-d37eb65d17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand previous to all data, paired and unpaired diversities\n",
    "d_to_metric = {\n",
    "    'alpha': 'shannon_entropy',\n",
    "    'paired_alpha': 'shannon_entropy_diff',\n",
    "    'paired_beta': 'Unweighted_Unifrac',\n",
    "}\n",
    "group_var = 'Adherece_antiinflam'\n",
    "\n",
    "dfd_to_merge = {}\n",
    "g_test = ['stool','stool_adh','saliva','saliva_adh']\n",
    "for g in g_test:\n",
    "    dfd_to_merge[g] = {}\n",
    "\n",
    "gs = []\n",
    "ds = []\n",
    "os = []\n",
    "stats = []\n",
    "ts = []\n",
    "ps = []\n",
    "\n",
    "arr = [gs,ds,os,stats,ts,ps]\n",
    "\n",
    "def append_results(arr, val):\n",
    "    for a,v in zip(arr,val):\n",
    "        a.append(v)\n",
    "    return arr\n",
    "\n",
    "# for each sample type, grab relevant mapping file\n",
    "# g_test = saliva, saliva_adh, stool, etc.\n",
    "for g in g_test:\n",
    "    # drop duplicates so you have sample mapping to adh\n",
    "    df_map_sub = type_to_df_map[g.split('_')[0]]\n",
    "    df_map_sub.index = df_map_sub.index.map(lambda x: x.split('.')[0].replace('-',''))\n",
    "    df_map_sub = df_map_sub.dropna(how='any',subset=group_var,axis=0)           \n",
    "    \n",
    "    # figure out which samples to keep\n",
    "    # i.e. samples that have a pre and post time point\n",
    "    keep = []\n",
    "    for i in list(df_map_sub.index.values):\n",
    "        if len(df_map_sub.loc[i,:]) == 2:\n",
    "            keep.append(i)\n",
    "\n",
    "    # get unique entires in sorted order\n",
    "    # at this point we are only concerned with differences in values, \n",
    "    # as we've dropped samples with only one endpoint val\n",
    "    save = []\n",
    "    [save.append(x) for x in keep if x not in save]\n",
    "    df_map_sub = df_map_sub.loc[save,:]\n",
    "\n",
    "    # this double populates as OAD001 is an index twice, so the diff fills to both the pre and post col\n",
    "    for o in outcomes:\n",
    "        df_map_sub[o + '_diff'] = df_map_sub[df_map_sub['Timepoint'] == 'post'][o] - df_map_sub[df_map_sub['Timepoint'] == 'pre'][o] \n",
    "\n",
    "    # here we keep only the pre, but everything is identical b/w pre and post\n",
    "    df_dropdup = df_map_sub[~df_map_sub.index.duplicated(keep='first')]\n",
    "\n",
    "    for d in d_to_metric:\n",
    "        # grab relevant diversity df\n",
    "        df_div = g_to_dfd[g][d]\n",
    "\n",
    "        # when associating alpha div vs outcomes, look at if starting adiv predicts outcome\n",
    "        if d == 'alpha':\n",
    "            df_div = df_div[df_div['Timepoints'] == 'pre']\n",
    "            df_div = df_div.set_index('SubjectID').drop('Timepoints',axis=1)\n",
    "            df_div = df_div.astype(float)\n",
    "\n",
    "        # merge with df of metadata var        \n",
    "        df_merge = pd.concat([df_dropdup,df_div],axis=1)\n",
    "\n",
    "        # drop na in barcodes if samples not sequenced both pre and post\n",
    "        df_merge = df_merge.dropna(how='any',subset='BarcodeSequence')\n",
    "\n",
    "        # build expandable arrays\n",
    "        values = list(df_merge[group_var].unique())\n",
    "        arr_list = [list(df_merge.groupby([group_var]).get_group(values[i])[d_to_metric[d]].values) for i in range(len(values))]\n",
    "    \n",
    "        # test difference of paired differences between two adherence groups (mod vs high only)\n",
    "        if len(g.split('_')) == 2:\n",
    "            s, p = scipy.stats.mannwhitneyu(arr_list[0], arr_list[1], nan_policy='omit')\n",
    "            arr = append_results(arr, [g,d,'adh','mwh',s,p])\n",
    "\n",
    "            s, p = scipy.stats.ttest_ind(arr_list[0], arr_list[1], nan_policy='omit')\n",
    "            arr = append_results(arr, [g,d,'adh','tt',s,p])\n",
    "\n",
    "        if len(g.split('_')) == 1: # tests across all 3 groups\n",
    "            s, p = scipy.stats.kruskal(*arr_list, nan_policy='omit')\n",
    "            arr = append_results(arr, [g,d,'adh','kw',s,p])\n",
    "\n",
    "            #f, p = scipy.stats.f_oneway(*arr_list, nan_policy='omit')\n",
    "            #print(f, p)\n",
    "\n",
    "        # separate plot\n",
    "        div_metric = d_to_metric[d]\n",
    "        df_merge[div_metric] = df_merge[div_metric].map(lambda x: float(x))\n",
    "        ax = sns.boxplot(data=df_merge, x=group_var, y=div_metric)\n",
    "        sns.swarmplot(data=df_merge, x=group_var, y=div_metric, palette='dark:grey')\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + 'outputs/Qiime2_' + g + '/' + d + '.pdf')\n",
    "        plt.close()          \n",
    "\n",
    "        # test association of div with outcomes\n",
    "        for o in outcomes:            \n",
    "            if o in 'PASE_light' or o in 'PASE_walk': # only 2\n",
    "                df_merge[o + '_quartiles'] = np.where(df_merge[o + '_diff']==0, 'bottom', 'top')\n",
    "            #elif o in 'PASE_walk': # only 3 \n",
    "            #    df_merge[o + '_quartiles'] = pd.qcut(df_merge[o + '_diff'].values, 3, labels = ['top','mid','bottom'])#, duplicates='drop')  \n",
    "            else:\n",
    "                df_merge[o + '_quartiles'] = pd.qcut(df_merge[o + '_diff'].values, 4, labels = ['top','midtop','midbot','bottom'])#, duplicates='drop')  \n",
    "                \n",
    "            # test association of adherence with pain outcomes\n",
    "            x,p,dof,ef = chisq_of_df_cols(df_merge, group_var, o + '_quartiles')\n",
    "            arr = append_results(arr, ['metadata','chisq',o,group_var,x,p])\n",
    "            \n",
    "            ax = sns.boxplot(data=df_merge, y=o + '_quartiles', x=div_metric)\n",
    "            sns.swarmplot(data=df_merge, y=o + '_quartiles', x=div_metric, palette='dark:grey')\n",
    "            sns.despine()\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/Qiime2_' + g + '/quartiles_' + o + '_' + d + '.pdf')\n",
    "            plt.close()          \n",
    "\n",
    "            # MWU and ttest for quartiles\n",
    "            u, p = scipy.stats.mannwhitneyu(df_merge[df_merge[o + '_quartiles'] == 'top'][div_metric].values, \n",
    "                                            df_merge[df_merge[o + '_quartiles'] == 'bottom'][div_metric].values, \n",
    "                                            nan_policy='omit')\n",
    "\n",
    "            arr = append_results(arr, [g,d,o,'mwu',t,p])\n",
    "\n",
    "            t, p = scipy.stats.ttest_ind(df_merge[df_merge[o + '_quartiles'] == 'top'][div_metric].values, \n",
    "                                            df_merge[df_merge[o + '_quartiles'] == 'bottom'][div_metric].values, \n",
    "                                            nan_policy='omit')\n",
    "\n",
    "            arr = append_results(arr, [g,d,o,'tt',t,p])\n",
    "            \n",
    "            # correlations (get df dropping nas) for metric and div metric not doing any diffs\n",
    "            if d == 'alpha':\n",
    "                df_corr = df_merge.loc[:,[div_metric,o]].dropna(how='any',axis=0)\n",
    "                r, p = scipy.stats.pearsonr(df_corr[div_metric].values, \n",
    "                                            df_corr[o].values)\n",
    "                \n",
    "                arr = append_results(arr, [g, d, o + '_nondiff', 'pearson', r, p])\n",
    "    \n",
    "                r, p = scipy.stats.spearmanr(df_corr[div_metric].values, \n",
    "                                            df_corr[o].values)\n",
    "                arr = append_results(arr, [g, d, o + '_nondiff', 'spearman', r, p])\n",
    "                ax = sns.scatterplot(data=df_corr, x=div_metric, y=o)\n",
    "                sns.despine()\n",
    "            \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(path + 'outputs/Qiime2_' + g + '/scatter_nondiff_' + o + '_' + d + '.pdf')\n",
    "                plt.close()          \n",
    "            \n",
    "            # correlations (get df dropping nas) for metric and div metric \n",
    "            df_corr = df_merge.loc[:,[div_metric,o + '_diff']].dropna(how='any',axis=0)\n",
    "            r, p = scipy.stats.pearsonr(df_corr[div_metric].values, \n",
    "                                        df_corr[o + '_diff'].values)\n",
    "            \n",
    "            arr = append_results(arr, [g, d, o, 'pearson', r, p])\n",
    "\n",
    "            r, p = scipy.stats.spearmanr(df_corr[div_metric].values, \n",
    "                                        df_corr[o + '_diff'].values)\n",
    "            arr = append_results(arr, [g, d, o, 'spearman', r, p])\n",
    "            ax = sns.scatterplot(data=df_corr, x=div_metric, y=o+'_diff')\n",
    "            sns.despine()\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + 'outputs/Qiime2_' + g + '/scatter_' + o + '_' + d + '.pdf')\n",
    "            plt.close()          \n",
    "\n",
    "            # do against paired outcomes\n",
    "\n",
    "        \n",
    "        # save results\n",
    "        dfd_to_merge[g][d] = df_merge\n",
    "\n",
    "        # export to Q2\n",
    "        df_q2_type = df_merge.set_index(['Together'])\n",
    "        q2_row = pd.Series(data=['categorical' for i in range(len(df_merge.columns))], \n",
    "                           index=list(df_merge.columns.values), dtype=str, name='#q2:types')\n",
    "        df_q2_type = pd.concat([q2_row.to_frame().T, df_q2_type])\n",
    "        df_q2_type.index.name = '#SampleID'\n",
    "        df_q2_type.index = df_q2_type.index.map(lambda x: x.split('.guma')[0])\n",
    "        df_q2_type.to_csv(path + 'inputs/qiime_mapping_file_' + d + '_' + g + '_outcomes.tsv', sep='\\t')\n",
    "\n",
    "df_results = pd.DataFrame.from_dict({\n",
    "    'group': gs,\n",
    "    'div': ds,\n",
    "    'outcome': os,\n",
    "    'statistic': stats,\n",
    "    'test_stat': ts,\n",
    "    'pval': ps\n",
    "})\n",
    "df_results.to_csv(path + 'outputs/df_results_diff.tsv', sep='\\t')\n",
    "\n",
    "# df_results.head()\n",
    "# the div==alpha results test whether pre-alpha div state associates with pain outcome changes (differences) quartiles\n",
    "# the div==paired_alpha and paired_beta test whether the alpha and betas change in a similar way with the pain outcome\n",
    "df_results[df_results['pval'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8c05d-208b-4b13-bac7-01bf00e4e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha div paired\n",
    "# paired beta, comparing intra-indiv difference pre_post to inter pre and inter post\n",
    "\n",
    "for g in g_test: #subgroups:\n",
    "    print(g)\n",
    "    df_alpha = pd.read_csv(path + 'outputs/Qiime2_' + g + '/metadata.tsv', sep='\\t', index_col=0)\n",
    "    df_alpha = df_alpha.drop('#q2:types')\n",
    "    df_alpha['SubjectID'] = df_alpha['Patient_ID'] + df_alpha['Study_ID']\n",
    "    df_alpha = df_alpha[['SubjectID', 'Timepoints', 'shannon_entropy']]\n",
    "    \n",
    "    # drop unpaired samples\n",
    "    s_remove = []\n",
    "    for s in list(df_alpha['SubjectID'].values):\n",
    "        if len(df_alpha[df_alpha['SubjectID'] == s]) != 2:\n",
    "            s_remove.append(s)\n",
    "    df_alpha = df_alpha.loc[~df_alpha['SubjectID'].isin(s_remove),:] # careful not to use ([s_remove])\n",
    "    \n",
    "    # set vars\n",
    "    alpha_metric = 'shannon_entropy'\n",
    "    group_var = 'Timepoints'\n",
    "    pair_var = 'SubjectID'\n",
    "    groups = ['pre','post']\n",
    "    \n",
    "    # get paired per indiv pair\n",
    "    pair_to_diff = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        df = df_alpha[df_alpha[pair_var] == p]\n",
    "        alpha_0 = float(df[df[group_var] == groups[0]][alpha_metric].values)\n",
    "        alpha_1 = float(df[df[group_var] == groups[1]][alpha_metric].values)\n",
    "        pair_to_diff[p] = alpha_0 - alpha_1\n",
    "    \n",
    "    df_paired_alpha = pd.DataFrame.from_dict(pair_to_diff, orient='index', columns=[alpha_metric + '_diff'])\n",
    "    \n",
    "    # one-sided t-test, n.s.; RA-UA values \n",
    "    t, p = scipy.stats.ttest_1samp(df_paired_alpha[alpha_metric + '_diff'],popmean=0)\n",
    "    print(t, p)\n",
    "    \n",
    "    s, p = scipy.stats.wilcoxon(df_paired_alpha[alpha_metric + '_diff'])\n",
    "    print(s, p)\n",
    "    \n",
    "    # separate\n",
    "    df_alpha[alpha_metric] = df_alpha[alpha_metric].map(lambda x: float(x))\n",
    "    ax = sns.boxplot(data=df_alpha, x=group_var, y=alpha_metric)\n",
    "    sns.swarmplot(data=df_alpha, x=group_var, y=alpha_metric, palette='dark:grey')\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + 'outputs/Qiime2_' + g + '/alpha.pdf')\n",
    "    plt.close()          \n",
    "\n",
    "    # now do beta\n",
    "    df_beta = pd.read_csv(path + 'outputs/Qiime2_' + g + '/core_metrics_results/distance-matrix.tsv',\n",
    "                          sep='\\t', index_col=0)\n",
    "    \n",
    "    # set vars\n",
    "    alpha_metric = 'shannon_entropy'\n",
    "    group_var = 'Timepoints'\n",
    "    pair_var = 'SubjectID'\n",
    "    groups = ['pre','post']\n",
    "    g0, g1 = groups[0], groups[1]\n",
    "    \n",
    "    # grab twin to pair dict\n",
    "    pair_to_ids = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        df = df_alpha[df_alpha[pair_var] == p]\n",
    "        id_0 = str(df[df[group_var] == g0].index.values[0])\n",
    "        id_1 = str(df[df[group_var] == g1].index.values[0])\n",
    "        pair_to_ids[p] = (id_0, id_1)\n",
    "    \n",
    "    # get distances for each twin pair per beta div matrix    \n",
    "    pair_to_dist = {}\n",
    "    for p in list(df_alpha[pair_var].values):\n",
    "        id_0, id_1 = pair_to_ids[p]\n",
    "        pair_to_dist[p] = df_beta.loc[id_0, id_1]\n",
    "    \n",
    "    df_paired_beta = pd.DataFrame.from_dict(pair_to_dist, orient='index', columns=['Unweighted_Unifrac'])\n",
    "    \n",
    "    # grab inter RA distances\n",
    "    # this is from unweighted_Timepoint_significance.qzv -> download as tsv\n",
    "    df_raw = pd.read_csv(path + 'outputs/Qiime2_' + g + '/raw_data.tsv', \n",
    "                         sep='\\t', index_col=0)\n",
    "    df_0 = df_raw[df_raw['Group1'] == g0]\n",
    "    df_0 = df_0[df_0['Group2'] == g0]\n",
    "    df_1 = df_raw[df_raw['Group1'] == g1]\n",
    "    df_1 = df_1[df_1['Group2'] == g1]\n",
    "    \n",
    "    # compare distances\n",
    "    inter_twin = df_paired_beta['Unweighted_Unifrac'].values\n",
    "    inter_0 = df_0['Distance'].values\n",
    "    inter_1 = df_1['Distance'].values\n",
    "    \n",
    "    u, p = scipy.stats.mannwhitneyu(inter_twin, inter_0)\n",
    "    #print(u, p)\n",
    "    \n",
    "    t, p = scipy.stats.ttest_ind(inter_twin, inter_1)\n",
    "    #print(t, p)\n",
    "    \n",
    "    t, p = scipy.stats.ttest_ind(inter_0, inter_1)\n",
    "    # print(t, p)\n",
    "    \n",
    "    f, p = scipy.stats.f_oneway(inter_0, inter_1, inter_twin)\n",
    "    print(f, p)\n",
    "    \n",
    "    category = ['intra_twin_pair']*len(inter_twin) + ['inter_' + g0 + '_only']*len(inter_0) + ['inter_' + g1 + '_only']*len(inter_1)\n",
    "    distances = list(inter_twin) + list(inter_0) + list(inter_1)\n",
    "    df_dist = pd.DataFrame(data=np.array([category,distances]).T, columns=['category','distance'])\n",
    "    df_dist['distance'] = df_dist['distance'].astype(float)\n",
    "    df_dist.to_csv(path + 'outputs/Qiime2_' + g + '/inter_intra_beta_dist.tsv',sep='\\t')\n",
    "                         \n",
    "    sns.boxplot(data=df_dist, x='category', y='distance')\n",
    "    sns.swarmplot(data=df_dist, x='category', y='distance', color='black')\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + 'outputs/Qiime2_' + g + '/beta.pdf')\n",
    "    plt.close()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1fdd5-b592-4a4e-a5d4-3cefc2d28c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metabolome testing\n",
    "gdt_to_df = {}\n",
    "for g in ['saliva','stool']:\n",
    "    gdt_to_df[g] = {}\n",
    "    for d in ['paired_beta']: # just arbitrarily \n",
    "        gdt_to_df[g][d] = {}\n",
    "        df_merge = dfd_to_merge[g][d]\n",
    "        df_meta = pd.read_csv(path + 'inputs/' + g + '_normalized.csv')\n",
    "\n",
    "        # split on timepoint\n",
    "        for t in ['Baseline','After diet']:\n",
    "            df = df_meta[df_meta['Time'] == t]\n",
    "            df = df.set_index('Study_ID')\n",
    "            df.index = df.index.map(lambda x: x.split('_')[0])\n",
    "            df = df.drop('Time',axis=1)\n",
    "            gdt_to_df[g][d][t] = df\n",
    "\n",
    "        gdt_to_df[g][d]['diff'] = gdt_to_df[g][d]['After diet'] - gdt_to_df[g][d]['Baseline']\n",
    "\n",
    "gdt_to_df[g][d][t].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bdc64-9d32-4724-a6d6-398a44c3eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differnetial metabolome profiling\n",
    "gs = []\n",
    "os = []\n",
    "ts = []\n",
    "stats = []\n",
    "xs = []\n",
    "ss = []\n",
    "ps = []\n",
    "\n",
    "arr = [gs,os,ts,stats,xs,ss,ps]\n",
    "\n",
    "for g in ['stool','saliva']:\n",
    "    for d in ['paired_beta']:\n",
    "        # test before and after\n",
    "        for tp in ['diff']:\n",
    "            #df = gdt_to_df['stool']['paired_beta']['diff']\n",
    "            df = gdt_to_df[g][d][tp].copy()\n",
    "            df_map = dfd_to_merge[g + '_adh'][d]\n",
    "            df = df.dropna(axis=0,how='all')\n",
    "            #df_map.index = df_map.index.map(lambda x: x.split('.')[0].replace('-',''))\n",
    "            #df = pd.concat([df_map['WOMAC_pain_quartiles'],df],axis=1)\n",
    "            for x in list(df.columns.values):\n",
    "                t, p = scipy.stats.ttest_1samp(df[x].values, popmean=0,nan_policy='omit')\n",
    "                arr = append_results(arr, [g,d,'post-pre','pairedt',x,t,p])\n",
    "\n",
    "                if p<0.05:\n",
    "                    df_post = gdt_to_df[g][d]['After diet'].copy() #.rename(columns={x: x + '_post'}, inplace=True)\n",
    "                    df_post['Timepoint'] = 'post'\n",
    "                    df_pre = gdt_to_df[g][d]['Baseline'].copy() # .rename(columns={x: x + '_pre'}, inplace=True)\n",
    "                    df_pre['Timepoint'] = 'pre'\n",
    "                    \n",
    "                    df_plot = pd.concat([df_post, df_pre], axis=0)\n",
    "                    sns.boxplot(data=df_plot, x='Timepoint', y=x)\n",
    "                    sns.swarmplot(data=df_plot, x='Timepoint', y=x, color='black')\n",
    "                    sns.despine()\n",
    "                \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(path + 'outputs/Qiime2_' + g + '/meta_' + x.replace('/','fslash') + '_differential.pdf')\n",
    "                    plt.close()    \n",
    "\n",
    "        \n",
    "        for o in outcomes:\n",
    "            # build expandable arrays; first get possible values of outcome\n",
    "            outcome_var = o + '_quartiles'\n",
    "            df_map = dfd_to_merge[g + '_adh'][d][outcome_var]\n",
    "            # drop na\n",
    "            df_map = df_map.dropna(how='any')\n",
    "            values = list(df_map.unique())\n",
    "\n",
    "            for t in ['Baseline','After diet','diff']:\n",
    "                # merge with pre post or diff metabolome data\n",
    "                df = pd.concat([df_map, gdt_to_df[g][d][t]],axis=1)\n",
    "            \n",
    "                # remove outcome var from iterations\n",
    "                df_x = df.drop(outcome_var,axis=1)\n",
    "                for x in list(df_x.columns.values):\n",
    "                    df_test = df.loc[:,[outcome_var,x]]\n",
    "                    #arr_list = [list(df_test.groupby([outcome_var]).get_group(values[i])[x].values) for i in range(len(values))]\n",
    "                \n",
    "                    #W, p = scipy.stats.kruskal(*arr_list)#, nan_policy='omit')\n",
    "                    u, p = scipy.stats.mannwhitneyu(df_test[df_test[outcome_var] == 'top'][x].values, \n",
    "                                                    df_test[df_test[outcome_var] == 'bottom'][x].values, \n",
    "                                                    nan_policy='omit')\n",
    "                            \n",
    "                    if p<0.05:\n",
    "                        sns.boxplot(data=df_test, x=outcome_var, y=x)\n",
    "                        sns.swarmplot(data=df_test, x=outcome_var, y=x, color='black')\n",
    "                        sns.despine()\n",
    "                    \n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(path + 'outputs/Qiime2_' + g + '/meta_' + x.replace('/','fslash') + '_' + t + '_' + o + '.pdf')\n",
    "                        plt.close()    \n",
    "\n",
    "                    # arr = append_results(arr, [g,o,t,'kw',x,u,p])\n",
    "                    arr = append_results(arr, [g,o,t,'mwu',x,u,p])\n",
    "        \n",
    "\n",
    "df_results = pd.DataFrame.from_dict({\n",
    "    'group': gs,\n",
    "    'outcome': os,\n",
    "    'timepoint': ts,\n",
    "    'statistic': stats,\n",
    "    'metabolite': xs,\n",
    "    'test_stat': ss,\n",
    "    'pval': ps\n",
    "})\n",
    "df_results.to_csv(path + 'outputs/df_results_meta_diff.tsv', sep='\\t')\n",
    "\n",
    "# df_results.head()\n",
    "# df = df_results[df_results['statistic'] == 'kw']\n",
    "df = df_results[df_results['statistic'] == 'mwu']\n",
    "df[df['pval'] < 0.05]\n",
    "\n",
    "#df = df_results[df_results['statistic'] == 'kw']\n",
    "#df = df.dropna(subset='pval')\n",
    "#df['FDR_bh'] = scipy.stats.false_discovery_control(df['pval'].values)\n",
    "#df = df[df['FDR_bh'] < 0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b0295-49c7-4525-b14e-d73dbaf81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differential metabolites post vs pre\n",
    "df = df_results[df_results['pval'] < 0.05]\n",
    "df = df[df['statistic'] == 'pairedt']\n",
    "# df = df[df['outcome'].isin(['VAS_Pt','WOMAC_pain'])]\n",
    "print(len(df))\n",
    "# df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5699b-8d26-478d-ac5a-d5eee7cf4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['pval'] < 0.05]\n",
    "df[df['outcome'].isin(['VAS_Pt','WOMAC_pain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e0ee0-93f2-4f45-9f7e-15f2dbc8bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform random forest \n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict,  KFold,  LeaveOneOut, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for g in ['stool','saliva']:\n",
    "    print(g)\n",
    "    for d in ['paired_beta']:\n",
    "        # test before and after\n",
    "        for t in ['diff']:\n",
    "            # diagnosis = df_num['BinDiag']\n",
    "            df = gdt_to_df[g][d][t]\n",
    "            df_map = dfd_to_merge[g + '_adh'][d]\n",
    "            outcome_var = 'VAS_Pt' + '_quartiles'\n",
    "            df = pd.concat([df_map[outcome_var], df], axis=1)\n",
    "            df = df.dropna(how='any',axis=0)\n",
    "            df = df[df[outcome_var].isin(['top','bottom'])]\n",
    "            df[outcome_var] = (df[outcome_var] == 'bottom').astype(int)\n",
    "            \n",
    "            # get columns of interest\n",
    "            features = [x.strip() for x in list(df.columns.values)]\n",
    "            \n",
    "            # subset df_rf\n",
    "            df = df[features]\n",
    "                        \n",
    "            # separate data and labels\n",
    "            X, y = df.drop(outcome_var,axis=1).values, df[outcome_var].values\n",
    "            \n",
    "            # set fold split\n",
    "            kf = LeaveOneOut()\n",
    "            # kf = KFold(n_splits=3)\n",
    "            \n",
    "            # try SVC\n",
    "            # clf = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)\n",
    "            clf = SVC(probability=True, random_state=42)\n",
    "            all_y = []\n",
    "            all_probs=[]\n",
    "            for train, test in kf.split(X, y):\n",
    "                all_y.append(y[test])\n",
    "                all_probs.append(clf.fit(X[train], y[train]).predict_proba(X[test])[:,1])\n",
    "            all_y = np.array(all_y)\n",
    "            all_probs = np.array(all_probs)\n",
    "            \n",
    "            fpr, tpr, thresholds = roc_curve(all_y,all_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print('SVC')\n",
    "            print(roc_auc)\n",
    "                \n",
    "            # try RF\n",
    "            # clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            all_y = []\n",
    "            all_probs=[]\n",
    "            for train, test in kf.split(X, y):\n",
    "                all_y.append(y[test])\n",
    "                all_probs.append(clf.fit(X[train], y[train]).predict_proba(X[test])[:,1])\n",
    "            all_y = np.array(all_y)\n",
    "            all_probs = np.array(all_probs)\n",
    "            \n",
    "            fpr, tpr, thresholds = roc_curve(all_y,all_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print('RF')\n",
    "            print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb1a97-0633-4567-84a3-c801f85e9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial correlation\n",
    "outcome_var = 'VAS_Pt' + '_quartiles'\n",
    "outcome_var = 'VAS_Pt'\n",
    "\n",
    "for g in ['stool','saliva']:\n",
    "    print(g)\n",
    "    for d in ['paired_beta']:\n",
    "        # test before and after\n",
    "        for t in ['diff']:\n",
    "            # diagnosis = df_num['BinDiag']\n",
    "            df = gdt_to_df[g][d][t]\n",
    "            df_map = dfd_to_merge[g + '_adh'][d]\n",
    "            df = pd.concat([df_map[outcome_var], df], axis=1)\n",
    "            df = df.dropna(how='any',axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af265f0-151a-4d4d-8d4c-f8ee8678bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import re\n",
    "df = df.copy()\n",
    "#df = df.rename(columns = lambda x: x.replace(':', 'bc'))\n",
    "#df = df.rename(columns = lambda x: x.replace(' ', 'space'))\n",
    "#df = df.rename(columns = lambda x: x.replace('/', 'fs'))\n",
    "#df = df.rename(columns = lambda x: x.replace('(', 'fp'))\n",
    "#df = df.rename(columns = lambda x: x.replace(')', 'bp'))\n",
    "#df = df.rename(columns = lambda x: x.replace('.', 'p'))\n",
    "#df = df.rename(columns = lambda x: x.replace('_', 'us'))\n",
    "#df = df.rename(columns = lambda x: x.replace('[', 'fb'))\n",
    "# df = df.rename(columns = lambda x: x.replace(']', 'bb'))\n",
    "df = df.rename(columns = lambda x: re.sub(r'\\W+', '', x))\n",
    "\n",
    "indep = list(df.columns.values)[1]\n",
    "for x in list(df.columns.values)[2:]:\n",
    "    indep = indep + ' + ' + x\n",
    "formula = str(outcome_var) + ' ~ ' + indep # \"A ~ B + C\"\n",
    "result = sm.ols(formula=formula, data=df).fit()\n",
    "# print(result.params)\n",
    "# print(result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9178b8-4227-4ad6-93ec-b3f75f704e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "X, Y = df.iloc[:,1:], df.iloc[:,0]\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X, Y)\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652c773-a126-44c9-aa01-05a07992d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame.from_dict({'var':list(df.columns.values)[1:],'coef':clf.coef_})\n",
    "tdf[tdf['coef'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308287d-4650-471f-8a49-5f15adc258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#rng = np.random.RandomState(0)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=rng)\n",
    "X_train, X_test, y_train, y_test = X, X, Y, Y\n",
    "\n",
    "pcr = make_pipeline(StandardScaler(), PCA(n_components=1), LinearRegression())\n",
    "pcr.fit(X_train, y_train)\n",
    "pca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n",
    "\n",
    "pls = PLSRegression(n_components=1)\n",
    "pls.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].scatter(pca.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
    "axes[0].scatter(\n",
    "    pca.transform(X_test), pcr.predict(X_test), alpha=0.3, label=\"predictions\"\n",
    ")\n",
    "axes[0].set(\n",
    "    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[1].scatter(pls.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
    "axes[1].scatter(\n",
    "    pls.transform(X_test), pls.predict(X_test), alpha=0.3, label=\"predictions\"\n",
    ")\n",
    "axes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\n",
    "print(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc5009-cb76-45d8-b3b2-4a0ce7620350",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\n",
    "pca_2.fit(X_train, y_train)\n",
    "print(f\"PCR r-squared with 2 components {pca_2.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bd4bb-68a2-4cd3-8646-1a9d74f713e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pls.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa7b98-67d0-4e52-af09-3c86264c45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iMic\n",
    "df = pd.read_csv(path + 'outputs/Qiime2_stool_adh/level-6.csv', index_col=0)\n",
    "\n",
    "# filter out metadata\n",
    "for x in list(df.columns.values):\n",
    "    if 'k__' not in x:\n",
    "        df = df.drop(x,axis=1)\n",
    "\n",
    "# normalize to 0-1\n",
    "df = df.div(df.sum(axis=1),axis=0).T\n",
    "\n",
    "df.to_csv(path + 'outputs/Qiime2_stool_adh/otu_table_L6.csv',index_label=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2193443-d92a-4a67-b9bb-81caf3e9c4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
